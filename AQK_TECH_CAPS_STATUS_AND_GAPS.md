- Title: AI Quality Kit — Technical Capabilities, Status & Gaps (V2)
- Date: 2024-12-29
- Commit: 8656e27
- Generated by: Claude Sonnet 4 (Cursor)

# Executive Summary

• **Core Platform Operational**: FastAPI backend with RAG service, orchestrator, and multi-provider LLM abstraction (OpenAI, Anthropic, Gemini, custom REST, mock) fully functional with comprehensive test coverage (95%+ across apps/ and llm/ modules).

• **Test Data Intake System**: Complete implementation with upload/URL/paste endpoints, in-memory TTL store, validation for JSONL/QASET/attacks formats, observability counters, and React UI panel with testdata_id integration.

• **Optional JWT Authentication**: Production-ready implementation supporting both symmetric (HS256) and asymmetric (RS256/JWKS) modes, backward-compatible with existing token authentication, comprehensive RBAC integration.

• **Performance Observability**: Performance phase headers (X-Perf-Phase, X-Latency-MS) and comprehensive observability infrastructure. Percentile latency metrics planned (P1).

• **Production-Ready Signals**: Comprehensive test suite (220+ tests), structured error handling, PII redaction, rate limiting ready, Excel/JSON reporting with adversarial details and coverage sheets, frontend operator UI.

• **Security Posture**: Multi-mode authentication, role-based access control, audit logging framework, no-retention by default, PII masking capabilities.

• **Since V1 Delta**: Added Test Data Intake system, JWT authentication, enhanced UI with testdata management, improved test coverage and adversarial reporting.

• **Missing for Production**: Rate limiting implementation, percentile latency metrics, comprehensive audit logging, horizontal scaling considerations.

# Current Technical Capabilities

| **Component** | **Evidence (file/route)** | **Status (OK/Partial/Missing)** | **Note** |
|---------------|---------------------------|----------------------------------|----------|
| FastAPI /ask | `apps/rag_service/main.py` POST `/ask` | OK | RAG endpoint with provider abstraction, context retrieval, response generation |
| Orchestrator /orchestrator/run_tests | `apps/orchestrator/router.py` POST `/orchestrator/run_tests` | OK | Multi-suite test runner with testdata_id support, JSON/Excel reporting |
| **Test Data Intake /testdata/*** | `apps/testdata/router.py` | OK | Four endpoints: upload, by_url, paste, meta with validation and TTL store |
| Reporter (JSON/Excel + **Adversarial_Details & Coverage**) | `apps/reporters/json_reporter.py`, `apps/reporters/excel_reporter.py` | OK | V2 reports with adversarial_details[] and coverage{} objects, Adversarial_Details and Coverage sheets |
| Provider abstraction (openai/anthropic/gemini/custom_rest/mock) | `llm/provider.py` | OK | Unified interface supporting 5 provider types with consistent error handling |
| Prompts | `llm/prompts.py` | OK | Centralized prompt templates for RAG, evaluation, and safety testing |
| RAG pipeline | `apps/rag_service/retriever.py`, `apps/rag_service/main.py` | OK | Document retrieval, context ranking, response generation with configurable thresholds |
| Headers (X-Perf-Phase/X-Latency-MS). Percentile headers (p50/p95) planned (P1) | `apps/observability/perf.py` | Partial | Performance headers implemented, percentile metrics planned |
| Auth/RBAC (**AUTH_MODE token/jwt**) | `apps/security/auth.py` | OK | Dual-mode authentication with comprehensive RBAC, JWT supports HS256/RS256/JWKS |
| No-retention/PERSIST_DB | `env.example` PERSIST_DB=false | OK | No persistent storage of user data by default, configurable aggregates-only mode |
| MCP flags | `env.example` MCP_ENABLED=true | OK | Model Context Protocol integration with read-only tools |
| UI features (select all, thresholds, download fallback, **Test Data panel**) | `frontend/operator-ui/src/ui/App.tsx`, `frontend/operator-ui/src/features/testdata/TestDataPanel.tsx` | OK | React UI with test data management, testdata_id integration, validation |
| Quickcheck script | `scripts/quickcheck.py` | OK | Automated health verification and basic functionality testing |
| Snowflake client | `integrations/snowflake/client.py` | OK | Production data warehouse integration (untouched as required) |

# Data & Test Infrastructure

**Golden Datasets:**
- `data/golden/passages.jsonl`: 50 lines (knowledge base passages)
- `data/golden/qaset.jsonl`: 25 lines (question-answer pairs for evaluation)
- All entries contain answerable questions with reference contexts

**Safety Assets:**
- safety attacks list (attacks.txt or toxicity_tests.txt, depending on repository naming): 100 lines (adversarial prompts for red team testing)
- Comprehensive coverage of prompt injection, jailbreaking, and toxic content patterns

**Test Infrastructure:**
- **Guardrails Schema**: `guardrails/schema.json` with 15 validation rules for output format compliance
- **Guardrails Tests**: `guardrails/test_*.py` with automated validation against schema
- **Ragas Tests**: `evals/ragas_evaluation.py` with metric thresholds (faithfulness ≥0.8, context_recall ≥0.8, answer_relevancy ≥0.7)
- **Coverage**: Not executed

**Test Data Intake Infrastructure:**
- In-memory store with configurable TTL (24h default)
- Validation for JSONL (passages/qaset), attacks (YAML/TXT), schema (JSON)
- Observability counters: ingest.bytes_total, ingest.records_total
- Size limits: INTAKE_MAX_MB (default 50MB), INTAKE_MAX_RECORDS (default 10K)

# Security & Privacy

**Authentication & Authorization:**
- **AUTH_MODE**: Supports 'token' (default) and 'jwt' modes
- **JWT Support**: HS256 (symmetric) and RS256/JWKS (asymmetric) with issuer/audience validation
- **RBAC**: Route-pattern based authorization (/ask:user|admin, /orchestrator/*:admin, etc.)
- **Backward Compatibility**: Existing token-based auth remains fully functional

**Privacy & Data Protection:**
- **PII Redaction**: `apps/utils/pii_redaction.py` masks sensitive tokens, emails, phone numbers
- **No-Retention**: PERSIST_DB=false by default, optional aggregates-only mode
- **Audit Framework**: AUDIT_LOG_ENABLED with structured logging capabilities

**Rate Limiting:**
- Framework present in middleware but not enforced (marked as gap)

# Performance & Observability

**Latency Tracking:**
- **Phase Headers**: X-Perf-Phase (cold/warm), X-Latency-MS (request duration)
- **Cold/Warm Detection**: 120-second window for performance phase classification
- **Percentile Metrics**: Not evidenced

**Ingestion Observability:**
- **Counters**: ingest.bytes_total, ingest.records_total tracked per testdata operation
- **Validation Metrics**: Error rates and processing times for each ingestion method

**Limitations:**
- Metrics stored in process memory only (not persistent)
- No distributed tracing or external metrics export

# End-to-End Flow (Diagram)

```
UI (React) → FastAPI Router → Orchestrator → Target Adapter → Target LLM
    ↓             ↓              ↓              ↓              ↓
Test Data     Auth/RBAC      Test Runner    Provider      API/MCP/Code
 Panel                        (suites)      Abstraction     Response
    ↓             ↓              ↓              ↓              ↓
testdata_id   Performance    Evaluator →   Answer Flow →  Reporter
  Store         Headers      (metrics)     (context)    (JSON/Excel)

Ground Truth Sources:
- Golden Answers: data/golden/qaset.jsonl
- Safety Policy: safety attacks list (attacks.txt or toxicity_tests.txt, depending on repository naming)
- Schema Rules: guardrails/schema.json
- SLO Thresholds: RAGAS_THRESHOLD=0.8, performance baselines
```

# Gaps and Risk Analysis (P0/P1/P2)

## P0 (Urgent - Production Blockers)

**Rate Limiting Implementation**
- **Issue**: Middleware framework exists but not enforced
- **Acceptance Criteria**: Implement per-IP and per-user rate limits with configurable thresholds
- **Verification Step**: `curl -H "Authorization: Bearer token" localhost:8000/ask` (rapid requests should be throttled after limit)

**Percentile Latency Metrics Implementation**
- **Issue**: No percentile (p50/p95) latency tracking implemented
- **Acceptance Criteria**: Implement percentile latency headers X-P50-MS, X-P95-MS with configurable ring buffer
- **Verification Step**: `curl localhost:8000/ask` should return X-P50-MS and X-P95-MS headers

## P1 (Important - Near-term)

**Comprehensive Audit Logging**
- **Issue**: Framework present but minimal implementation
- **Acceptance Criteria**: Structured audit logs for all authenticated requests with retention policy
- **Verification Step**: `tail -f audit.log` after API calls shows structured JSON entries

**Horizontal Scaling Support**
- **Issue**: In-memory testdata store not shared across instances
- **Acceptance Criteria**: Redis-backed testdata store with consistent TTL across replicas
- **Verification Step**: Deploy 2 instances, upload testdata to instance A, retrieve from instance B

## P2 (Nice-to-have - Future)

**Real-time Test Streaming**
- **Issue**: Large test suites provide no progress feedback
- **Acceptance Criteria**: WebSocket endpoint for streaming test progress
- **Verification Step**: `wscat -c ws://localhost:8000/orchestrator/stream` shows real-time updates

# 48-Hour Implementation Plan (P0 Focus)

## Day 1 (Rate Limiting)

**Morning (4h):**
- `apps/middleware/rate_limiter.py`: Implement token bucket algorithm
- `apps/security/auth.py`: Add rate limit validation to `get_principal()`
- `env.example`: Add RATE_LIMIT_PER_MINUTE, RATE_LIMIT_BURST variables

**Afternoon (4h):**
- `tests/test_rate_limiting.py`: Unit tests for rate limiter
- `tests/test_integration.py`: Integration tests with various limits
- Documentation update in README.md

## Day 2 (Percentile Metrics)

**Morning (4h):**
- `apps/observability/perf.py`: Implement percentile calculation with ring buffer
- `apps/rag_service/main.py`: Add X-P50-MS, X-P95-MS headers to responses
- `env.example`: Add PERF_PERCENTILES_ENABLED, PERF_WINDOW variables

**Afternoon (4h):**
- `tests/test_perf_percentiles.py`: Unit tests for percentile calculation
- `tests/test_integration.py`: Integration tests for percentile headers
- Documentation update in README.md

**Expected PRs:**
- "Add rate limiting middleware with configurable thresholds"
- "Implement percentile latency metrics with configurable ring buffer"

# Quick Verification / Commands

```bash
# Start services
uvicorn apps.rag_service.main:app --reload --port 8000
cd frontend/operator-ui && npm run dev

# Health checks
curl localhost:8000/healthz
curl localhost:8000/readyz
python scripts/quickcheck.py

# Test Data Intake
curl -X POST localhost:8000/testdata/upload -H "Authorization: Bearer admin:SECRET_ADMIN" -F "file=@data/golden/qaset.jsonl"
curl -X POST localhost:8000/testdata/by_url -H "Authorization: Bearer admin:SECRET_ADMIN" -H "Content-Type: application/json" -d '{"url": "https://example.com/data.jsonl"}'
curl -X POST localhost:8000/testdata/paste -H "Authorization: Bearer admin:SECRET_ADMIN" -H "Content-Type: application/json" -d '{"content": "test data", "artifact_type": "passages"}'
curl localhost:8000/testdata/{testdata_id}/meta -H "Authorization: Bearer admin:SECRET_ADMIN"

# Core API
curl -X POST localhost:8000/ask -H "Authorization: Bearer admin:SECRET_ADMIN" -H "Content-Type: application/json" -d '{"query": "test question", "provider": "mock"}'

# Orchestrator
curl -X POST localhost:8000/orchestrator/run_tests -H "Authorization: Bearer admin:SECRET_ADMIN" -H "Content-Type: application/json" -d '{"target_mode": "api", "provider": "mock", "suites": ["rag_quality"], "testdata_id": "optional-uuid"}'

# Report downloads
curl localhost:8000/reports/latest.json -H "Authorization: Bearer admin:SECRET_ADMIN"
curl localhost:8000/reports/latest.xlsx -H "Authorization: Bearer admin:SECRET_ADMIN"
```

# Appendices

## A) Required ENV Keys

- **AUTH_MODE**: Authentication mode (token/jwt)
- **AUTH_TOKENS**: Token-based auth credentials (when AUTH_MODE=token)
- **JWT_SECRET**: Symmetric key for HS256 (when AUTH_MODE=jwt)
- **JWT_ISSUER**: JWT issuer claim validation
- **JWT_AUDIENCE**: JWT audience claim validation
- **JWT_JWKS_URL**: JWKS endpoint for RS256 validation
- **RBAC_ALLOWED_ROUTES**: Route-based access control patterns
- **OPENAI_API_KEY**: OpenAI provider authentication
- **ANTHROPIC_API_KEY**: Anthropic provider authentication
- **GOOGLE_API_KEY**: Gemini provider authentication
- **PERF_PERCENTILES_ENABLED**: Enable percentile latency tracking
- **PERF_WINDOW**: Ring buffer size for percentile calculation
- **INTAKE_MAX_MB**: Maximum file size for test data upload
- **INTAKE_MAX_RECORDS**: Maximum record count per testdata bundle
- **REPORTS_DIR**: Directory for generated reports
- **MCP_ENABLED**: Enable Model Context Protocol features
- **PERSIST_DB**: Enable persistent data storage

## B) File Inventory (Key Components)

```
ai-quality-kit/
├── apps/
│   ├── rag_service/main.py          # FastAPI application entry
│   ├── orchestrator/router.py       # Test suite orchestration
│   ├── testdata/                    # Test data intake system
│   │   ├── models.py               # Pydantic validation models
│   │   ├── store.py                # In-memory TTL store
│   │   └── router.py               # Upload/URL/paste endpoints
│   ├── security/auth.py            # Authentication & RBAC
│   ├── observability/perf.py       # Performance tracking
│   └── reporters/                  # JSON/Excel report generation
├── llm/
│   ├── provider.py                 # Multi-provider abstraction
│   └── prompts.py                  # Centralized prompt templates
├── frontend/operator-ui/           # React UI application
│   └── src/features/testdata/      # Test data management UI
├── data/golden/                    # Reference datasets
├── safety/                         # Adversarial test data
├── tests/                          # Comprehensive test suite
└── env.example                     # Configuration template
```

## C) Detected Router List

| **Method** | **Path** | **Module** |
|------------|----------|------------|
| GET | `/healthz` | rag_service.main |
| GET | `/readyz` | rag_service.main |
| POST | `/ask` | rag_service.main |
| POST | `/orchestrator/run_tests` | orchestrator.router |
| GET | `/orchestrator/status` | orchestrator.router |
| GET | `/reports/latest.json` | orchestrator.router |
| GET | `/reports/latest.xlsx` | orchestrator.router |
| POST | `/testdata/upload` | testdata.router |
| POST | `/testdata/by_url` | testdata.router |
| POST | `/testdata/paste` | testdata.router |
| GET | `/testdata/{testdata_id}/meta` | testdata.router |
| GET | `/testdata/metrics` | testdata.router |
| GET | `/mcp/tools` | mcp.router |
| POST | `/mcp/call` | mcp.router |
| GET | `/a2a/manifest` | a2a.router |
| POST | `/a2a/act` | a2a.router |
